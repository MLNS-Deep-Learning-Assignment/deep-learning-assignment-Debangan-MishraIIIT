{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataloader import get_loaders\n",
    "from models import CNNRegressor, CNNClassifier, VGGRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'trained_models' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = \"trained_models\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor Modelling\n",
    "\n",
    "The given task is such that we are given an input image, and we are predicting the sum of the numbers represented in the image. If there is no guarantee as to whether the test set contains exactly 4 numbers, the typical classifier approach will likely fail. As a result, we can modify the network to act as a regressor with a single output and the output value being rounded to get an integer representation. Hence, during training, the model will use MSE Loss (typically used for regression tasks) and will round the number to closest integer during inference to calculate accuracy. \n",
    "\n",
    "Note that as a result of this approach, a low MSE Loss need not translate to a good accuracy. Consider for example, our model predicts 24.6 as an output while the ground truth is 24. In this case, the MSE Loss will be low, but the accuracy need not be as rounding changes the output. Also note that it is not possible to train the model based on rounded outputs as rounding is a step function with zero gradient almost everywhere making it useless for learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model, criterion, optimizer, epochs, device, type=\"regressor\"):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            if type==\"classifier\":\n",
    "                labels = labels.to(torch.long)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_acc = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                if type==\"classifier\":\n",
    "                    outputs = F.softmax(outputs, dim=-1)\n",
    "                    outputs = torch.argmax(outputs, dim=1)\n",
    "                accuracy = (torch.sum(torch.round(outputs)==labels)*100/len(labels)).item()\n",
    "                val_acc += accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Accuracy: {val_acc/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_fraction = 0.8\n",
    "batch_size = 64\n",
    "seed = 42\n",
    "epochs = 15\n",
    "learning_rate = 1e-3\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 34.8493, Val Accuracy: 8.4386\n",
      "Epoch 2/15, Train Loss: 21.6001, Val Accuracy: 9.4692\n",
      "Epoch 3/15, Train Loss: 15.5664, Val Accuracy: 10.5773\n",
      "Epoch 4/15, Train Loss: 12.6231, Val Accuracy: 11.4971\n",
      "Epoch 5/15, Train Loss: 10.8117, Val Accuracy: 11.9293\n",
      "Epoch 6/15, Train Loss: 9.6357, Val Accuracy: 9.6133\n",
      "Epoch 7/15, Train Loss: 9.2238, Val Accuracy: 12.9266\n",
      "Epoch 8/15, Train Loss: 8.2191, Val Accuracy: 12.3670\n",
      "Epoch 9/15, Train Loss: 7.4315, Val Accuracy: 12.0124\n",
      "Epoch 10/15, Train Loss: 7.4809, Val Accuracy: 12.1786\n",
      "Epoch 11/15, Train Loss: 6.2272, Val Accuracy: 11.9238\n",
      "Epoch 12/15, Train Loss: 5.8341, Val Accuracy: 12.6274\n",
      "Epoch 13/15, Train Loss: 6.3204, Val Accuracy: 12.9045\n",
      "Epoch 14/15, Train Loss: 5.6288, Val Accuracy: 12.8823\n",
      "Epoch 15/15, Train Loss: 5.2671, Val Accuracy: 10.6494\n"
     ]
    }
   ],
   "source": [
    "model = CNNRegressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader, val_loader = get_loaders(train_fraction, batch_size, seed)\n",
    "train_model(train_loader, val_loader, model, criterion, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy of Regressor on Validation Split: 10.65%\n",
      "Overall Accuracy of Regressor on Training Split: 18.16%\n"
     ]
    }
   ],
   "source": [
    "## Calculating overall accuracy of the validation set\n",
    "model.eval()\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        correct_count = (torch.sum(torch.round(outputs)==y)).item()\n",
    "        total_len += len(y)\n",
    "        total_correct += correct_count\n",
    "accuracy = (total_correct/total_len)*100\n",
    "print(f\"Overall Accuracy of Regressor on Validation Split: {accuracy:.2f}%\")\n",
    "\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        correct_count = (torch.sum(torch.round(outputs)==y)).item()\n",
    "        total_len += len(y)\n",
    "        total_correct += correct_count\n",
    "accuracy = (total_correct/total_len)*100\n",
    "print(f\"Overall Accuracy of Regressor on Training Split: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./trained_models/cnnregressor_{seed}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Modelling\n",
    "\n",
    "When we are assuming that the input image will always have 4 numbers as that is the only value appearing in the dataset. In that case, we can model the network as a classifier with the outputs ranging from 0 to 36. For this case, we can use the cross entropy loss which is commonly used for classification task. Empirically we observe that the model performance on the validation split is lower for the classifier than it is for the regressor. This might be because regression operates in the continuous space (just like the input) and this possibly allows for a smoother optimization. It is not possible to verify why the regressor works better than the classifier case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 3.1989, Val Accuracy: 6.9204\n",
      "Epoch 2/15, Train Loss: 3.0899, Val Accuracy: 7.7405\n",
      "Epoch 3/15, Train Loss: 2.9005, Val Accuracy: 9.2531\n",
      "Epoch 4/15, Train Loss: 2.7468, Val Accuracy: 9.2586\n",
      "Epoch 5/15, Train Loss: 2.5727, Val Accuracy: 8.6990\n",
      "Epoch 6/15, Train Loss: 2.3447, Val Accuracy: 8.7600\n",
      "Epoch 7/15, Train Loss: 2.0550, Val Accuracy: 7.9566\n",
      "Epoch 8/15, Train Loss: 1.7215, Val Accuracy: 8.1449\n",
      "Epoch 9/15, Train Loss: 1.3895, Val Accuracy: 8.3666\n",
      "Epoch 10/15, Train Loss: 1.1216, Val Accuracy: 7.9289\n",
      "Epoch 11/15, Train Loss: 0.9226, Val Accuracy: 7.8125\n",
      "Epoch 12/15, Train Loss: 0.6945, Val Accuracy: 7.6186\n",
      "Epoch 13/15, Train Loss: 0.5235, Val Accuracy: 6.7764\n",
      "Epoch 14/15, Train Loss: 0.4084, Val Accuracy: 7.0368\n",
      "Epoch 15/15, Train Loss: 0.3506, Val Accuracy: 7.6961\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader, val_loader = get_loaders(train_fraction, batch_size, seed)\n",
    "train_model(train_loader, val_loader, model, criterion, optimizer, epochs, device, type=\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy of Classifer on Validation Split: 7.70%\n",
      "Overall Accuracy of Classifer on Training Split: 68.08%\n"
     ]
    }
   ],
   "source": [
    "## Calculating overall accuracy of the validation set\n",
    "model.eval()\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        outputs = F.softmax(outputs, dim=-1)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        correct_count = (torch.sum(torch.round(outputs)==y)).item()\n",
    "        total_len += len(y)\n",
    "        total_correct += correct_count\n",
    "accuracy = (total_correct/total_len)*100\n",
    "print(f\"Overall Accuracy of Classifer on Validation Split: {accuracy:.2f}%\")\n",
    "\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        outputs = F.softmax(outputs, dim=-1)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        correct_count = (torch.sum(torch.round(outputs)==y)).item()\n",
    "        total_len += len(y)\n",
    "        total_correct += correct_count\n",
    "accuracy = (total_correct/total_len)*100\n",
    "print(f\"Overall Accuracy of Classifer on Training Split: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./trained_models/cnnclassifier_{seed}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same behaviour was observed across seeds (0, 10, 42), indicating a consistent behaviour. Also note that early stopping based on validation loss has not been implemented for the models (even though the validation accuracy is higher at some early stages) to ensure the train loss is sufficiently low. Training for further epochs leads to a decrease in the validation accuracy indicating overfitting. Hence, the number of epochs was chosen to be 15. Note that the training accuracy is very high for the classifier approach, but the validation is not, indicating a possibility that this approach is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning With VGG\n",
    "\n",
    "Owing to the success of the regressor approach, a third type of network was created using transfer learning using VGG (a popular CNN network which achieves fairly high accuracy across various tasks). The convolutional backbone and the average pooling layers of the network are frozen and a classifier is added on top which has one output as the previous CNN regressor case. Note that a convolutional layer is also prepended to ensure that the MNIST dataset with one channel is compatible with the VGG network that is trained on 3 channel RGB images. This approach is computationally more expensive, but was tried out to see whether the pretraining of the VGG leads to better results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m      5\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m get_loaders(train_fraction, batch_size, seed)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_loader, val_loader, model, criterion, optimizer, epochs, device, type)\u001b[0m\n\u001b[0;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 15\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VGGRegressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader, val_loader = get_loaders(train_fraction, batch_size, seed)\n",
    "train_model(train_loader, val_loader, model, criterion, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy of Regressor on Validation Split: 10.23%\n",
      "Overall Accuracy of Regressor on Training Split: 9.98%\n"
     ]
    }
   ],
   "source": [
    "## Calculating overall accuracy of the validation set\n",
    "model.eval()\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        correct_count = (torch.sum(torch.round(outputs)==y)).item()\n",
    "        total_len += len(y)\n",
    "        total_correct += correct_count\n",
    "accuracy = (total_correct/total_len)*100\n",
    "print(f\"Overall Accuracy of Regressor on Validation Split: {accuracy:.2f}%\")\n",
    "\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        correct_count = (torch.sum(torch.round(outputs)==y)).item()\n",
    "        total_len += len(y)\n",
    "        total_correct += correct_count\n",
    "accuracy = (total_correct/total_len)*100\n",
    "print(f\"Overall Accuracy of Regressor on Training Split: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./trained_models/vggregressor_{seed}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Across Seeds\n",
    "\n",
    "#### Validation Accuracy\n",
    "\n",
    "| Seed | CNN Regressor | CNN Classifier | VGG Regressor |\n",
    "|------|---------------|----------------|---------------|\n",
    "| 0    | 11.42%        | 6.70%          | 10.23%        |\n",
    "| 10   | 10.92%        | 8.00%          | 0             |\n",
    "| 42   | 10.65%        | 7.70%          | 9.12%         |\n",
    "\n",
    "#### Training Accuracy\n",
    "\n",
    "| Seed | CNN Regressor | CNN Classifier | VGG Regressor |\n",
    "|------|---------------|----------------|---------------|\n",
    "| 0    | 21.90%        | 52.36%         | 9.98%         |\n",
    "| 10   | 22.22%        | 76.92%         | 0             |\n",
    "| 42   | 18.16%        | 68.08%         | 9.29%         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
